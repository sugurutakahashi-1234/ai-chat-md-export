# Transkripsi WhisperKit & SpeechFramework

æ—¥ä»˜: 2024-05-17

---

## ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼
*2024/5/17 18:54:46*

ã“ã®ç”»é¢ã§WhisperKitã«ã‚ˆã‚‹æ–‡å­—èµ·ã“ã—ã¨ã€Apple ã® Speech Framework ã«ã‚ˆã‚‹æ–‡å­—èµ·ã“ã—ã‚’æ¯”è¼ƒã§ãã‚‹ã‚ˆã†ãªç”»é¢ã‚’ä½œã£ã¦ã»ã—ã„ã€‚

Apple ã® Speech Framework ã«ã‚ˆã‚‹æ–‡å­—èµ·ã“ã—æœªå®Ÿè£…ã§ã‚ã‚ŠSpeechFrameworkranscriptionDriverã¨ã—ã¦ã€å®Ÿè£…ã—ã¦ã»ã—ã„

import Combine
import Foundation
@preconcurrency import WhisperKit

extension TranscriptionResult: Identifiable {
    public var id: String {
        timings.pipelineStart.description
    }
}

@MainActor
final class TranscriptionPresenter: ObservableObject {
    @Published var transcriptionResults: \[TranscriptionResult\] = \[\]
    @Published var isLoading: Bool = false
    @Published var appError: AppError?
    @Published var showAlert: Bool = false
    
    let audioUrl: URL
    
    private let transcriptionDriver: TranscriptionDriver = TranscriptionDriver()

    init(audioUrl: URL) {
        self.audioUrl = audioUrl
    }

    func onApper() {}
    
    func onTapTranscribe() async {
        isLoading = true
        defer {
            isLoading = false
        }

        do {
            transcriptionResults = try await transcriptionDriver.transcribe(audioUrl: audioUrl)
        } catch {
            OSLogger.errorLog("Failed to read audio files: \\(error)")
            appError = error.toAppError
            showAlert = true
        }
    }
}
â€¨import SwiftUI

@MainActor
struct TranscriptionView: View {
    @StateObject private var presenter: TranscriptionPresenter

    init(audioUrl: URL) {
        \_presenter = .init(wrappedValue: .init(audioUrl: audioUrl))
    }
    
    var body: some View {
        VStack {
            Button {
                Task {
                    await presenter.onTapTranscribe()
                }
                
            } label: {
                Text("ç¿»è¨³ã™ã‚‹")
            }
            
            List(presenter.transcriptionResults) { result in
                Text(result.text)
            }
        }
        .navigationTitle("æ–‡å­—èµ·ã“ã—")
        .overlayLoading(isPresented: $presenter.isLoading, isGestureReactable: true)
        .onAppear {
            presenter.onApper()
        }
    }
}


final class TranscriptionDriver: Sendable {
    // WhisperKit ã®ç”Ÿæˆã«æ™‚é–“ãŒã‹ã‹ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ä½¿ã„å›ãã†ã¨ã—ã¦ã„ã‚‹ãŒä¸è¦ã‹ã‚‚ã—ã‚Œãªã„
    nonisolated(unsafe) private static var sharedWhisperKit: WhisperKit?

    init() {
        Task {
            do {
                OSLogger.debugLog("init WhisperKit")
                TranscriptionDriver.sharedWhisperKit = try await WhisperKit()
                OSLogger.debugLog("create WhisperKit")
            } catch {
                OSLogger.debugLog("Failed to initialize WhisperKit: \\(error)")
            }
        }
    }
    
    private func getWhisperKit() async throws -&gt; WhisperKit {
        if let whisper = TranscriptionDriver.sharedWhisperKit {
            return whisper
        } else {
            let whisper = try await WhisperKit()
            TranscriptionDriver.sharedWhisperKit = whisper
            return whisper
        }
    }

    // TODO: TranscriptionResult ã®æŠ½è±¡åŒ–
    func transcribe(audioUrl: URL) async throws -&gt; \[TranscriptionResult\] {
        OSLogger.debugLog("start transcribe")
        let whisper = try await getWhisperKit()
        let transcriptionResults: \[TranscriptionResult\] = try await whisper.transcribe(audioPath: audioUrl.path, decodeOptions:DecodingOptions.init(language: "ja"))
        OSLogger.debugLog("language: \\(transcriptionResults.first!.language)")
        OSLogger.debugLog("text: \\(transcriptionResults.first!.text)")
        OSLogger.debugLog("allWords: \\(transcriptionResults.first!.allWords)")
        OSLogger.debugLog("segments: \\(transcriptionResults.first!.segments)")
        OSLogger.debugLog("timings: \\(transcriptionResults.first!.timings)")
        OSLogger.debugLog("fullPipeline: \\(transcriptionResults.first!.timings.fullPipeline)")
        return transcriptionResults
    }
}

---

## ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ
*2024/5/17 18:55:13*

User wants to create a screen to compare transcriptions using WhisperKit and Apple's Speech Framework, and they need an implementation for SpeechFrameworkranscriptionDriver to handle transcriptions with the Apple Speech Framework.

---

## ğŸ”§ ãƒ„ãƒ¼ãƒ«
*2024/5/17 18:55:13*

Model set context updated.

---

## ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ
*2024/5/17 18:55:13*

To implement a screen that allows comparing transcriptions using WhisperKit and Apple's Speech Framework, you need to create a `SpeechFrameworkTranscriptionDriver` and modify your `TranscriptionPresenter` and `TranscriptionView` to handle both transcription methods. Hereâ€™s how you can do it:

1. **Implement `SpeechFrameworkTranscriptionDriver`:**

```swift
import Foundation
import Speech

final class SpeechFrameworkTranscriptionDriver: Sendable {
    func transcribe(audioUrl: URL) async throws -> [TranscriptionResult] {
        let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP"))!
        let request = SFSpeechURLRecognitionRequest(url: audioUrl)
        
        return try await withCheckedThrowingContinuation { continuation in
            recognizer.recognitionTask(with: request) { result, error in
                if let error = error {
                    continuation.resume(throwing: error)
                    return
                }
                
                if let result = result {
                    let transcriptionResult = TranscriptionResult(
                        text: result.bestTranscription.formattedString,
                        // Assuming you have the necessary details to populate these fields
                        language: "ja",
                        allWords: result.bestTranscription.segments.map { $0.substring },
                        segments: [], // Populate with necessary segments data
                        timings: Timings(pipelineStart: Date(), fullPipeline: Date())
                    )
                    continuation.resume(returning: [transcriptionResult])
                }
            }
        }
    }
}
```

2. **Update `TranscriptionPresenter` to handle both drivers:**

```swift
@MainActor
final class TranscriptionPresenter: ObservableObject {
    @Published var whisperTranscriptionResults: [TranscriptionResult] = []
    @Published var speechFrameworkTranscriptionResults: [TranscriptionResult] = []
    @Published var isLoading: Bool = false
    @Published var appError: AppError?
    @Published var showAlert: Bool = false
    
    let audioUrl: URL
    
    private let whisperTranscriptionDriver = TranscriptionDriver()
    private let speechFrameworkTranscriptionDriver = SpeechFrameworkTranscriptionDriver()

    init(audioUrl: URL) {
        self.audioUrl = audioUrl
    }

    func onApper() {}
    
    func onTapTranscribe() async {
        isLoading = true
        defer {
            isLoading = false
        }

        do {
            async let whisperResults = whisperTranscriptionDriver.transcribe(audioUrl: audioUrl)
            async let speechFrameworkResults = speechFrameworkTranscriptionDriver.transcribe(audioUrl: audioUrl)
            
            whisperTranscriptionResults = try await whisperResults
            speechFrameworkTranscriptionResults = try await speechFrameworkResults
        } catch {
            OSLogger.errorLog("Failed to transcribe audio files: \(error)")
            appError = error.toAppError
            showAlert = true
        }
    }
}
```

3. **Update `TranscriptionView` to display results from both drivers:**

```swift
import SwiftUI

@MainActor
struct TranscriptionView: View {
    @StateObject private var presenter: TranscriptionPresenter

    init(audioUrl: URL) {
        _presenter = .init(wrappedValue: .init(audioUrl: audioUrl))
    }
    
    var body: some View {
        VStack {
            Button {
                Task {
                    await presenter.onTapTranscribe()
                }
                
            } label: {
                Text("ç¿»è¨³ã™ã‚‹")
            }
            
            List {
                Section(header: Text("WhisperKit Transcriptions")) {
                    ForEach(presenter.whisperTranscriptionResults) { result in
                        Text(result.text)
                    }
                }
                Section(header: Text("Speech Framework Transcriptions")) {
                    ForEach(presenter.speechFrameworkTranscriptionResults) { result in
                        Text(result.text)
                    }
                }
            }
        }
        .navigationTitle("æ–‡å­—èµ·ã“ã—")
        .overlayLoading(isPresented: $presenter.isLoading, isGestureReactable: true)
        .onAppear {
            presenter.onApper()
        }
    }
}
```

This implementation will allow you to compare the transcriptions from WhisperKit and Apple's Speech Framework side by side. Make sure you have the necessary error handling and UI adjustments to fit your needs.

---

## ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼
*2024/5/17 19:02:36*

SpeechFrameworkTranscriptionDriver ã¯ func transcribe(audioUrl: URL) async throws -&gt; SFSpeechRecognitionRequest ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ä¸€æ—¦å¤§ä¸ˆå¤«

---

## ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ
*2024/5/17 19:02:54*

äº†è§£ã—ã¾ã—ãŸã€‚ã§ã¯ã€`SpeechFrameworkTranscriptionDriver` ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«ã—ã¦ã€`SFSpeechRecognitionRequest` ã‚’è¿”ã™ã‚ˆã†ã«å¤‰æ›´ã—ã¾ã™ã€‚ã¾ãŸã€`TranscriptionPresenter` ã¨ `TranscriptionView` ã‚’å¯¾å¿œã•ã›ã¾ã™ã€‚

### SpeechFrameworkTranscriptionDriver

```swift
import Foundation
import Speech

final class SpeechFrameworkTranscriptionDriver: Sendable {
    func transcribe(audioUrl: URL) async throws -> SFSpeechRecognitionRequest {
        let request = SFSpeechURLRecognitionRequest(url: audioUrl)
        return request
    }
}
```

### TranscriptionPresenter ã®æ›´æ–°

```swift
@MainActor
final class TranscriptionPresenter: ObservableObject {
    @Published var whisperTranscriptionResults: [TranscriptionResult] = []
    @Published var speechFrameworkTranscriptionRequest: SFSpeechRecognitionRequest?
    @Published var isLoading: Bool = false
    @Published var appError: AppError?
    @Published var showAlert: Bool = false
    
    let audioUrl: URL
    
    private let whisperTranscriptionDriver = TranscriptionDriver()
    private let speechFrameworkTranscriptionDriver = SpeechFrameworkTranscriptionDriver()

    init(audioUrl: URL) {
        self.audioUrl = audioUrl
    }

    func onApper() {}
    
    func onTapTranscribe() async {
        isLoading = true
        defer {
            isLoading = false
        }

        do {
            async let whisperResults = whisperTranscriptionDriver.transcribe(audioUrl: audioUrl)
            async let speechFrameworkRequest = speechFrameworkTranscriptionDriver.transcribe(audioUrl: audioUrl)
            
            whisperTranscriptionResults = try await whisperResults
            speechFrameworkTranscriptionRequest = try await speechFrameworkRequest
        } catch {
            OSLogger.errorLog("Failed to transcribe audio files: \(error)")
            appError = error.toAppError
            showAlert = true
        }
    }
}
```

### TranscriptionView ã®æ›´æ–°

```swift
import SwiftUI

@MainActor
struct TranscriptionView: View {
    @StateObject private var presenter: TranscriptionPresenter

    init(audioUrl: URL) {
        _presenter = .init(wrappedValue: .init(audioUrl: audioUrl))
    }
    
    var body: some View {
        VStack {
            Button {
                Task {
                    await presenter.onTapTranscribe()
                }
                
            } label: {
                Text("ç¿»è¨³ã™ã‚‹")
            }
            
            List {
                Section(header: Text("WhisperKit Transcriptions")) {
                    ForEach(presenter.whisperTranscriptionResults) { result in
                        Text(result.text)
                    }
                }
                Section(header: Text("Speech Framework Transcriptions")) {
                    if let request = presenter.speechFrameworkTranscriptionRequest {
                        // Assuming you handle the Speech Framework transcription display logic here
                        Text("Speech Framework Request ready for processing")
                    } else {
                        Text("No transcriptions available")
                    }
                }
            }
        }
        .navigationTitle("æ–‡å­—èµ·ã“ã—")
        .overlayLoading(isPresented: $presenter.isLoading, isGestureReactable: true)
        .onAppear {
            presenter.onApper()
        }
    }
}
```

ã“ã®å¤‰æ›´ã«ã‚ˆã‚Šã€`SpeechFrameworkTranscriptionDriver` ã¯ `SFSpeechRecognitionRequest` ã‚’è¿”ã™ã‚ˆã†ã«ãªã‚Šã€`TranscriptionPresenter` ã¨ `TranscriptionView` ã¯ã“ã‚Œã‚’å—ã‘å–ã£ã¦é©åˆ‡ã«è¡¨ç¤ºã—ã¾ã™ã€‚ `Speech Framework` ã®æ–‡å­—èµ·ã“ã—çµæœã®è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ã¯ã€ã•ã‚‰ã«è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

---
