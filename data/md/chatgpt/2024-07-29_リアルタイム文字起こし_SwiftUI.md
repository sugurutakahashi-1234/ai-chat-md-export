# リアルタイム文字起こし SwiftUI

Date: 2024-07-29

---

## 👤 User
*2024-07-29 16:35:03*

SwiftUIでリアルタイム文字起こそい

---

## 🤖 Assistant
*2024-07-29 16:35:38*

SwiftUIでリアルタイム文字起こしを実装するには、`Speech`フレームワークを使用することができます。以下の手順に従って、基本的な実装を行うことができます。

### 1. 必要な権限を取得

まず、アプリが音声認識を使用できるようにするために、`Info.plist`に以下のエントリを追加します。

```xml
<key>NSSpeechRecognitionUsageDescription</key>
<string>音声認識のためにマイクを使用します。</string>
```

### 2. 必要なインポート

`Speech`フレームワークをインポートします。

```swift
import SwiftUI
import Speech
```

### 3. 音声認識の設定

音声認識の許可を要求し、音声認識を開始するためのクラスを作成します。

```swift
class SpeechRecognizer: ObservableObject {
    private var speechRecognizer = SFSpeechRecognizer()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine = AVAudioEngine()
    
    @Published var transcript: String = ""
    
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            switch authStatus {
            case .authorized:
                print("Authorized")
            case .denied:
                print("Denied")
            case .restricted:
                print("Restricted")
            case .notDetermined:
                print("Not Determined")
            @unknown default:
                fatalError()
            }
        }
    }
    
    func startTranscribing() {
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        
        guard let recognitionRequest = recognitionRequest else {
            fatalError("Unable to create an SFSpeechAudioBufferRecognitionRequest object")
        }
        
        guard let inputNode = audioEngine.inputNode else {
            fatalError("Audio engine has no input node")
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
            if let result = result {
                DispatchQueue.main.async {
                    self.transcript = result.bestTranscription.formattedString
                }
            }
            
            if let error = error {
                print("Error recognizing speech: \(error.localizedDescription)")
                self.stopTranscribing()
            }
        }
        
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { (buffer, when) in
            self.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
        } catch {
            print("AudioEngine couldn't start because of an error.")
        }
    }
    
    func stopTranscribing() {
        audioEngine.stop()
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask = nil
    }
}
```

### 4. SwiftUIビューの作成

音声認識を制御するためのSwiftUIビューを作成します。

```swift
struct ContentView: View {
    @ObservedObject var speechRecognizer = SpeechRecognizer()
    
    var body: some View {
        VStack {
            Text(speechRecognizer.transcript)
                .padding()
                .border(Color.gray, width: 1)
            
            Button(action: {
                self.speechRecognizer.startTranscribing()
            }) {
                Text("Start Transcribing")
            }
            .padding()
            
            Button(action: {
                self.speechRecognizer.stopTranscribing()
            }) {
                Text("Stop Transcribing")
            }
            .padding()
        }
        .onAppear {
            self.speechRecognizer.requestAuthorization()
        }
    }
}
```

### 5. アプリの起動

アプリを起動し、音声認識をテストします。

この実装は基本的なものであり、実際のアプリケーションではエラーハンドリングやUIの改善など、さらに多くの考慮が必要です。

---

## 👤 User
*2024-07-29 16:36:12*

AVAudioRecorder を使いたい

---

## 🤖 Assistant
*2024-07-29 16:36:51*

`AVAudioRecorder`を使用して音声を録音し、その音声をリアルタイムで文字起こしする実装を行うことができます。以下は、`AVAudioRecorder`を使った実装例です。

### 1. 必要な権限を取得

まず、アプリが音声録音を使用できるようにするために、`Info.plist`に以下のエントリを追加します。

```xml
<key>NSMicrophoneUsageDescription</key>
<string>マイクを使用します。</string>
<key>NSSpeechRecognitionUsageDescription</key>
<string>音声認識のためにマイクを使用します。</string>
```

### 2. 必要なインポート

`AVFoundation`と`Speech`フレームワークをインポートします。

```swift
import SwiftUI
import AVFoundation
import Speech
```

### 3. 音声録音と認識の設定

音声録音と音声認識を行うためのクラスを作成します。

```swift
class AudioRecorder: NSObject, ObservableObject, AVAudioRecorderDelegate {
    var audioRecorder: AVAudioRecorder!
    var audioFileURL: URL!
    
    func startRecording() {
        let settings = [
            AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
            AVSampleRateKey: 12000,
            AVNumberOfChannelsKey: 1,
            AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
        ]
        
        audioFileURL = getDocumentsDirectory().appendingPathComponent("recording.m4a")
        
        do {
            audioRecorder = try AVAudioRecorder(url: audioFileURL, settings: settings)
            audioRecorder.delegate = self
            audioRecorder.record()
        } catch {
            print("録音エラー: \(error.localizedDescription)")
        }
    }
    
    func stopRecording() {
        audioRecorder.stop()
    }
    
    private func getDocumentsDirectory() -> URL {
        let paths = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)
        return paths[0]
    }
}

class SpeechRecognizer: ObservableObject {
    private var speechRecognizer = SFSpeechRecognizer()
    private var recognitionRequest: SFSpeechURLRecognitionRequest?
    
    @Published var transcript: String = ""
    
    func transcribeAudioFile(url: URL) {
        recognitionRequest = SFSpeechURLRecognitionRequest(url: url)
        
        guard let recognitionRequest = recognitionRequest else {
            fatalError("Unable to create an SFSpeechURLRecognitionRequest object")
        }
        
        speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
            if let result = result {
                DispatchQueue.main.async {
                    self.transcript = result.bestTranscription.formattedString
                }
            }
            
            if let error = error {
                print("音声認識エラー: \(error.localizedDescription)")
            }
        }
    }
    
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            switch authStatus {
            case .authorized:
                print("音声認識許可")
            case .denied:
                print("音声認識拒否")
            case .restricted:
                print("音声認識制限")
            case .notDetermined:
                print("音声認識未決定")
            @unknown default:
                fatalError()
            }
        }
    }
}
```

### 4. SwiftUIビューの作成

録音と文字起こしを制御するためのSwiftUIビューを作成します。

```swift
struct ContentView: View {
    @StateObject var audioRecorder = AudioRecorder()
    @StateObject var speechRecognizer = SpeechRecognizer()
    
    var body: some View {
        VStack {
            Text(speechRecognizer.transcript)
                .padding()
                .border(Color.gray, width: 1)
            
            Button(action: {
                audioRecorder.startRecording()
            }) {
                Text("録音開始")
            }
            .padding()
            
            Button(action: {
                audioRecorder.stopRecording()
                speechRecognizer.transcribeAudioFile(url: audioRecorder.audioFileURL)
            }) {
                Text("録音停止と文字起こし開始")
            }
            .padding()
        }
        .onAppear {
            speechRecognizer.requestAuthorization()
        }
    }
}
```

### 5. アプリの起動

アプリを起動し、録音と音声認識をテストします。

この実装例では、録音された音声をファイルとして保存し、そのファイルを文字起こしするプロセスを示しています。リアルタイムでの文字起こしは行いませんが、録音終了後に文字起こしを開始することで、精度の高い結果を得ることができます。

---

## 👤 User
*2024-07-29 16:38:05*

すでに以下のような実装がある。別Driverを使って実装して欲しい

public final class RecordingDriver: Sendable {
    private nonisolated(unsafe) var audioRecorder: AVAudioRecorder?

    private nonisolated(unsafe) let isRecordingSubject = PassthroughSubject&lt;Bool, Never&gt;()
    public var isRecordingPublisher: AnyPublisher&lt;Bool, Never&gt; {
        isRecordingSubject.eraseToAnyPublisher()
    }

    /// second (s)
    private nonisolated(unsafe) let recordingTimeSubject = CurrentValueSubject&lt;Int, Never&gt;(0)
    public var recordingTimePublisher: AnyPublisher&lt;Int, Never&gt; {
        recordingTimeSubject.eraseToAnyPublisher()
    }

    /// Audio Power (0.0 - 1.0)
    private nonisolated(unsafe) var audioVolumeLevelSubject = CurrentValueSubject&lt;Double, Never&gt;(0.0)
    public var audioVolumeLevelPublisher: AnyPublisher&lt;Double, Never&gt; {
        audioVolumeLevelSubject.eraseToAnyPublisher()
    }

    // TODO: この実装はよくないが検証コードなのでそうしているだけであり、いつか削除する
    public nonisolated(unsafe) var isEnabledAudioSessionCache: Bool = false

    private nonisolated(unsafe) var recordingTimer = Set&lt;AnyCancellable&gt;()
    private nonisolated(unsafe) var meteringTimer = Set&lt;AnyCancellable&gt;()

    private nonisolated(unsafe) var meteringTimerInterval: TimeInterval = 0.05

    public init() {}

    public func setMeteringTimerInterval(_ interval: TimeInterval) {
        meteringTimerInterval = interval
    }

    public func startRecording(recordingConfig: RecordingConfig) async throws {
        let audioFilename = Constants.FilePath.audioDirectory
            .appendingPathComponent("\(Date.ios8601)")
            .appendingPathExtension(recordingConfig.audioFormat.fileExtension.rawValue)

        let settings = [
            AVFormatIDKey: Int(recordingConfig.audioFormat.audioFormatID),
            AVSampleRateKey: recordingConfig.audioSampleRate.rawValue,
            AVNumberOfChannelsKey: recordingConfig.audioChannel.rawValue,
            AVEncoderAudioQualityKey: recordingConfig.audioEncodeQuality.toAVAudioQuality.rawValue,
        ]

        await SystemSoundStaticDriver.play(systemSoundType: .startRecording)

        isEnabledAudioSessionCache = recordingConfig.isEnabledAudioSession
        if recordingConfig.isEnabledAudioSession {
            try AVAudioSession.sharedInstance().setCategory(
                recordingConfig.audioSessionCategory.toAVAudioSessionCategory,
                mode: recordingConfig.audioSessionMode.toAVAudioSessionMode,
                options: []
            )
            OSLogger.debugLog("Start: session active")
            try AVAudioSession.sharedInstance().setActive(true)
            OSLogger.debugLog("Completed: session active")
        }

        OSLogger.debugLog("Start: start recording")
        audioRecorder = try AVAudioRecorder(url: audioFilename, settings: settings)
        audioRecorder?.isMeteringEnabled = true
        audioRecorder?.prepareToRecord()

        // 上記までの処理はイニシャライザで行っても良いかもしれない
        audioRecorder?.record()
        isRecordingSubject.send(true)
        OSLogger.debugLog("Start: start recording")

        startRecordingTimer()
        startMeteringTimer()
    }

    public func stopRecording() async throws {
        guard let audioRecorder, audioRecorder.isRecording else {
            return
        }

        stopRecordingTimer()
        stopMeteringTimer()

        OSLogger.debugLog("Start: stop recording")
        audioRecorder.stop()
        isRecordingSubject.send(false)
        OSLogger.debugLog("Completed: stop recording")

        if isEnabledAudioSessionCache {
            OSLogger.debugLog("Start: session unactive")
            try AVAudioSession.sharedInstance().setActive(false)
            OSLogger.debugLog("Completed: session unactive")
        }

        await SystemSoundStaticDriver.play(systemSoundType: .stopRecording)
    }

    private func startRecordingTimer() {
        recordingTimeSubject.value = 0

        Timer.publish(every: 1, on: .main, in: .common).autoconnect()
            .sink { [weak self] _ in
                self?.recordingTimeSubject.value += 1
            }
            .store(in: &recordingTimer)
    }

    private func stopRecordingTimer() {
        recordingTimer.forEach { $0.cancel() }
    }

    private func startMeteringTimer() {
        Timer.publish(every: meteringTimerInterval, on: .main, in: .common).autoconnect()
            .compactMap { [weak self] _ in
                guard let self else {
                    return nil
                }
                audioRecorder?.updateMeters()
                return AudioHelpStaticDriver.normalized(decibel: Double(audioRecorder?.averagePower(forChannel: 0) ?? -80))
            }
            .assign(to: \.value, on: audioVolumeLevelSubject)
            .store(in: &meteringTimer)
    }

    private func stopMeteringTimer() {
        meteringTimer.forEach { $0.cancel() }
    }
}

---

## 🤖 Assistant
*2024-07-29 16:39:14*

User has a `RecordingDriver` class in Swift that uses `AVAudioRecorder` for recording audio and manages various recording states and metrics using Combine.

---

## 🔧 Tool
*2024-07-29 16:39:14*

Model set context updated.

---

## 🤖 Assistant
*2024-07-29 16:38:15*

別のドライバーを使用して、すでに存在する`RecordingDriver`クラスに基づいてリアルタイムの文字起こし機能を追加する実装を示します。`Speech`フレームワークを使用して文字起こしを行います。

まず、新しいドライバークラスとして`TranscriptionDriver`を作成し、その中で文字起こしの機能を実装します。次に、既存の`RecordingDriver`クラスにこのドライバーを統合します。

### 1. `TranscriptionDriver`の作成

```swift
import Speech
import Combine

public final class TranscriptionDriver: ObservableObject {
    private var speechRecognizer = SFSpeechRecognizer()
    private var recognitionRequest: SFS

---

## 👤 User
*2024-07-29 16:39:15*

各Driver は独立していて、それを使うUseCase の classを定義してそれで実装して欲しい。

---

## 🤖 Assistant
*2024-07-29 16:39:25*

User prefers each driver to be independent and wants to define a UseCase class to implement the functionality using these drivers.

---

## 🔧 Tool
*2024-07-29 16:39:25*

Model set context updated.

---

## 🤖 Assistant
*2024-07-29 16:39:25*

了解しました。各Driverを独立させたまま、UseCaseクラスを定義して、録音と文字起こしの機能を実装します。

### 1. TranscriptionDriver の作成

まず、音声認識を担当する `TranscriptionDriver` を作成します。

```swift
import Speech
import Combine

public final class TranscriptionDriver: ObservableObject {
    private var speechRecognizer = SFSpeechRecognizer()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    
    private let transcriptSubject = PassthroughSubject<String, Never>()
    public var transcriptPublisher: AnyPublisher<String, Never> {
        transcriptSubject.eraseToAnyPublisher()
    }
    
    public init() {}
    
    public func startTranscribing() {
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        
        guard let recognitionRequest = recognitionRequest else {
            fatalError("Unable to create an SFSpeechAudioBufferRecognitionRequest object")
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
            if let result = result {
                DispatchQueue.main.async {
                    self.transcriptSubject.send(result.bestTranscription.formattedString)
                }
            }
            
            if let error = error {
                print("Error recognizing speech: \(error.localizedDescription)")
                self.stopTranscribing()
            }
        }
    }
    
    public func stopTranscribing() {
        recognitionTask?.cancel()
        recognitionTask = nil
    }
    
    public func appendAudioData(_ buffer: AVAudioPCMBuffer) {
        recognitionRequest?.append(buffer)
    }
    
    public func requestAuthorization(completion: @escaping (Bool) -> Void) {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    completion(true)
                default:
                    completion(false)
                }
            }
        }
    }
}
```

### 2. UseCase クラスの作成

`RecordingDriver` と `TranscriptionDriver` を利用して録音と文字起こしの機能を統合する `RecordingUseCase` クラスを作成します。

```swift
import AVFoundation
import Combine

public final class RecordingUseCase: ObservableObject {
    private let recordingDriver: RecordingDriver
    private let transcriptionDriver: TranscriptionDriver
    
    private var audioEngine: AVAudioEngine
    private var audioInputNode: AVAudioInputNode?
    
    private var cancellables = Set<AnyCancellable>()
    
    @Published public var transcript: String = ""
    
    public init(recordingDriver: RecordingDriver, transcriptionDriver: TranscriptionDriver) {
        self.recordingDriver = recordingDriver
        self.transcriptionDriver = transcriptionDriver
        
        self.audioEngine = AVAudioEngine()
        
        setupBindings()
    }
    
    private func setupBindings() {
        transcriptionDriver.transcriptPublisher
            .sink { [weak self] transcript in
                self?.transcript = transcript
            }
            .store(in: &cancellables)
    }
    
    public func start() {
        transcriptionDriver.requestAuthorization { [weak self] authorized in
            guard authorized else {
                print("Speech recognition authorization denied")
                return
            }
            
            self?.transcriptionDriver.startTranscribing()
            self?.startRecording()
        }
    }
    
    private func startRecording() {
        let audioFormat = AVAudioFormat(standardFormatWithSampleRate: 44100.0, channels: 1)!
        audioInputNode = audioEngine.inputNode
        
        audioInputNode?.installTap(onBus: 0, bufferSize: 1024, format: audioFormat) { [weak self] buffer, when in
            self?.transcriptionDriver.appendAudioData(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
        } catch {
            print("AudioEngine couldn't start because of an error: \(error.localizedDescription)")
        }
        
        Task {
            try await recordingDriver.startRecording(recordingConfig: RecordingConfig())
        }
    }
    
    public func stop() {
        recordingDriver.stopRecording()
        transcriptionDriver.stopTranscribing()
        
        audioEngine.stop()
        audioInputNode?.removeTap(onBus: 0)
    }
}
```

### 3. SwiftUIビューの作成

録音と文字起こしを制御するための SwiftUI ビューを作成します。

```swift
import SwiftUI

struct ContentView: View {
    @StateObject var recordingUseCase = RecordingUseCase(
        recordingDriver: RecordingDriver(),
        transcriptionDriver: TranscriptionDriver()
    )
    
    var body: some View {
        VStack {
            Text(recordingUseCase.transcript)
                .padding()
                .border(Color.gray, width: 1)
            
            Button(action: {
                recordingUseCase.start()
            }) {
                Text("録音開始と文字起こし")
            }
            .padding()
            
            Button(action: {
                recordingUseCase.stop()
            }) {
                Text("録音停止")
            }
            .padding()
        }
    }
}
```

これで、録音とリアルタイムの文字起こしを行う機能が、独立したDriverクラスとUseCaseクラスを使用して実装されました。

---
