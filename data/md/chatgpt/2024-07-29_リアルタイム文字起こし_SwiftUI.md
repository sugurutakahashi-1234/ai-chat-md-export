# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã— SwiftUI

Date: 2024-07-29

---

## ğŸ‘¤ User
*2024-07-29 16:35:03*

SwiftUIã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ãã„

---

## ğŸ¤– Assistant
*2024-07-29 16:35:38*

SwiftUIã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè£…ã™ã‚‹ã«ã¯ã€`Speech`ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»¥ä¸‹ã®æ‰‹é †ã«å¾“ã£ã¦ã€åŸºæœ¬çš„ãªå®Ÿè£…ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚

### 1. å¿…è¦ãªæ¨©é™ã‚’å–å¾—

ã¾ãšã€ã‚¢ãƒ—ãƒªãŒéŸ³å£°èªè­˜ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã«ã€`Info.plist`ã«ä»¥ä¸‹ã®ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ã—ã¾ã™ã€‚

```xml
<key>NSSpeechRecognitionUsageDescription</key>
<string>éŸ³å£°èªè­˜ã®ãŸã‚ã«ãƒã‚¤ã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚</string>
```

### 2. å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ

`Speech`ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

```swift
import SwiftUI
import Speech
```

### 3. éŸ³å£°èªè­˜ã®è¨­å®š

éŸ³å£°èªè­˜ã®è¨±å¯ã‚’è¦æ±‚ã—ã€éŸ³å£°èªè­˜ã‚’é–‹å§‹ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚

```swift
class SpeechRecognizer: ObservableObject {
    private var speechRecognizer = SFSpeechRecognizer()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine = AVAudioEngine()
    
    @Published var transcript: String = ""
    
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            switch authStatus {
            case .authorized:
                print("Authorized")
            case .denied:
                print("Denied")
            case .restricted:
                print("Restricted")
            case .notDetermined:
                print("Not Determined")
            @unknown default:
                fatalError()
            }
        }
    }
    
    func startTranscribing() {
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        
        guard let recognitionRequest = recognitionRequest else {
            fatalError("Unable to create an SFSpeechAudioBufferRecognitionRequest object")
        }
        
        guard let inputNode = audioEngine.inputNode else {
            fatalError("Audio engine has no input node")
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
            if let result = result {
                DispatchQueue.main.async {
                    self.transcript = result.bestTranscription.formattedString
                }
            }
            
            if let error = error {
                print("Error recognizing speech: \(error.localizedDescription)")
                self.stopTranscribing()
            }
        }
        
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { (buffer, when) in
            self.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
        } catch {
            print("AudioEngine couldn't start because of an error.")
        }
    }
    
    func stopTranscribing() {
        audioEngine.stop()
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask = nil
    }
}
```

### 4. SwiftUIãƒ“ãƒ¥ãƒ¼ã®ä½œæˆ

éŸ³å£°èªè­˜ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®SwiftUIãƒ“ãƒ¥ãƒ¼ã‚’ä½œæˆã—ã¾ã™ã€‚

```swift
struct ContentView: View {
    @ObservedObject var speechRecognizer = SpeechRecognizer()
    
    var body: some View {
        VStack {
            Text(speechRecognizer.transcript)
                .padding()
                .border(Color.gray, width: 1)
            
            Button(action: {
                self.speechRecognizer.startTranscribing()
            }) {
                Text("Start Transcribing")
            }
            .padding()
            
            Button(action: {
                self.speechRecognizer.stopTranscribing()
            }) {
                Text("Stop Transcribing")
            }
            .padding()
        }
        .onAppear {
            self.speechRecognizer.requestAuthorization()
        }
    }
}
```

### 5. ã‚¢ãƒ—ãƒªã®èµ·å‹•

ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã€éŸ³å£°èªè­˜ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

ã“ã®å®Ÿè£…ã¯åŸºæœ¬çš„ãªã‚‚ã®ã§ã‚ã‚Šã€å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚„UIã®æ”¹å–„ãªã©ã€ã•ã‚‰ã«å¤šãã®è€ƒæ…®ãŒå¿…è¦ã§ã™ã€‚

---

## ğŸ‘¤ User
*2024-07-29 16:36:12*

AVAudioRecorder ã‚’ä½¿ã„ãŸã„

---

## ğŸ¤– Assistant
*2024-07-29 16:36:51*

`AVAudioRecorder`ã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ã‚’éŒ²éŸ³ã—ã€ãã®éŸ³å£°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ–‡å­—èµ·ã“ã—ã™ã‚‹å®Ÿè£…ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ä»¥ä¸‹ã¯ã€`AVAudioRecorder`ã‚’ä½¿ã£ãŸå®Ÿè£…ä¾‹ã§ã™ã€‚

### 1. å¿…è¦ãªæ¨©é™ã‚’å–å¾—

ã¾ãšã€ã‚¢ãƒ—ãƒªãŒéŸ³å£°éŒ²éŸ³ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã«ã€`Info.plist`ã«ä»¥ä¸‹ã®ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ã—ã¾ã™ã€‚

```xml
<key>NSMicrophoneUsageDescription</key>
<string>ãƒã‚¤ã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚</string>
<key>NSSpeechRecognitionUsageDescription</key>
<string>éŸ³å£°èªè­˜ã®ãŸã‚ã«ãƒã‚¤ã‚¯ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚</string>
```

### 2. å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ

`AVFoundation`ã¨`Speech`ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

```swift
import SwiftUI
import AVFoundation
import Speech
```

### 3. éŸ³å£°éŒ²éŸ³ã¨èªè­˜ã®è¨­å®š

éŸ³å£°éŒ²éŸ³ã¨éŸ³å£°èªè­˜ã‚’è¡Œã†ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚

```swift
class AudioRecorder: NSObject, ObservableObject, AVAudioRecorderDelegate {
    var audioRecorder: AVAudioRecorder!
    var audioFileURL: URL!
    
    func startRecording() {
        let settings = [
            AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
            AVSampleRateKey: 12000,
            AVNumberOfChannelsKey: 1,
            AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
        ]
        
        audioFileURL = getDocumentsDirectory().appendingPathComponent("recording.m4a")
        
        do {
            audioRecorder = try AVAudioRecorder(url: audioFileURL, settings: settings)
            audioRecorder.delegate = self
            audioRecorder.record()
        } catch {
            print("éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: \(error.localizedDescription)")
        }
    }
    
    func stopRecording() {
        audioRecorder.stop()
    }
    
    private func getDocumentsDirectory() -> URL {
        let paths = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)
        return paths[0]
    }
}

class SpeechRecognizer: ObservableObject {
    private var speechRecognizer = SFSpeechRecognizer()
    private var recognitionRequest: SFSpeechURLRecognitionRequest?
    
    @Published var transcript: String = ""
    
    func transcribeAudioFile(url: URL) {
        recognitionRequest = SFSpeechURLRecognitionRequest(url: url)
        
        guard let recognitionRequest = recognitionRequest else {
            fatalError("Unable to create an SFSpeechURLRecognitionRequest object")
        }
        
        speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
            if let result = result {
                DispatchQueue.main.async {
                    self.transcript = result.bestTranscription.formattedString
                }
            }
            
            if let error = error {
                print("éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: \(error.localizedDescription)")
            }
        }
    }
    
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            switch authStatus {
            case .authorized:
                print("éŸ³å£°èªè­˜è¨±å¯")
            case .denied:
                print("éŸ³å£°èªè­˜æ‹’å¦")
            case .restricted:
                print("éŸ³å£°èªè­˜åˆ¶é™")
            case .notDetermined:
                print("éŸ³å£°èªè­˜æœªæ±ºå®š")
            @unknown default:
                fatalError()
            }
        }
    }
}
```

### 4. SwiftUIãƒ“ãƒ¥ãƒ¼ã®ä½œæˆ

éŒ²éŸ³ã¨æ–‡å­—èµ·ã“ã—ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®SwiftUIãƒ“ãƒ¥ãƒ¼ã‚’ä½œæˆã—ã¾ã™ã€‚

```swift
struct ContentView: View {
    @StateObject var audioRecorder = AudioRecorder()
    @StateObject var speechRecognizer = SpeechRecognizer()
    
    var body: some View {
        VStack {
            Text(speechRecognizer.transcript)
                .padding()
                .border(Color.gray, width: 1)
            
            Button(action: {
                audioRecorder.startRecording()
            }) {
                Text("éŒ²éŸ³é–‹å§‹")
            }
            .padding()
            
            Button(action: {
                audioRecorder.stopRecording()
                speechRecognizer.transcribeAudioFile(url: audioRecorder.audioFileURL)
            }) {
                Text("éŒ²éŸ³åœæ­¢ã¨æ–‡å­—èµ·ã“ã—é–‹å§‹")
            }
            .padding()
        }
        .onAppear {
            speechRecognizer.requestAuthorization()
        }
    }
}
```

### 5. ã‚¢ãƒ—ãƒªã®èµ·å‹•

ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã€éŒ²éŸ³ã¨éŸ³å£°èªè­˜ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

ã“ã®å®Ÿè£…ä¾‹ã§ã¯ã€éŒ²éŸ³ã•ã‚ŒãŸéŸ³å£°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã€ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®æ–‡å­—èµ·ã“ã—ã¯è¡Œã„ã¾ã›ã‚“ãŒã€éŒ²éŸ³çµ‚äº†å¾Œã«æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã§ã€ç²¾åº¦ã®é«˜ã„çµæœã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

---

## ğŸ‘¤ User
*2024-07-29 16:38:05*

ã™ã§ã«ä»¥ä¸‹ã®ã‚ˆã†ãªå®Ÿè£…ãŒã‚ã‚‹ã€‚åˆ¥Driverã‚’ä½¿ã£ã¦å®Ÿè£…ã—ã¦æ¬²ã—ã„

public final class RecordingDriver: Sendable {
    private nonisolated(unsafe) var audioRecorder: AVAudioRecorder?

    private nonisolated(unsafe) let isRecordingSubject = PassthroughSubject&lt;Bool, Never&gt;()
    public var isRecordingPublisher: AnyPublisher&lt;Bool, Never&gt; {
        isRecordingSubject.eraseToAnyPublisher()
    }

    /// second (s)
    private nonisolated(unsafe) let recordingTimeSubject = CurrentValueSubject&lt;Int, Never&gt;(0)
    public var recordingTimePublisher: AnyPublisher&lt;Int, Never&gt; {
        recordingTimeSubject.eraseToAnyPublisher()
    }

    /// Audio Power (0.0 - 1.0)
    private nonisolated(unsafe) var audioVolumeLevelSubject = CurrentValueSubject&lt;Double, Never&gt;(0.0)
    public var audioVolumeLevelPublisher: AnyPublisher&lt;Double, Never&gt; {
        audioVolumeLevelSubject.eraseToAnyPublisher()
    }

    // TODO: ã“ã®å®Ÿè£…ã¯ã‚ˆããªã„ãŒæ¤œè¨¼ã‚³ãƒ¼ãƒ‰ãªã®ã§ãã†ã—ã¦ã„ã‚‹ã ã‘ã§ã‚ã‚Šã€ã„ã¤ã‹å‰Šé™¤ã™ã‚‹
    public nonisolated(unsafe) var isEnabledAudioSessionCache: Bool = false

    private nonisolated(unsafe) var recordingTimer = Set&lt;AnyCancellable&gt;()
    private nonisolated(unsafe) var meteringTimer = Set&lt;AnyCancellable&gt;()

    private nonisolated(unsafe) var meteringTimerInterval: TimeInterval = 0.05

    public init() {}

    public func setMeteringTimerInterval(_ interval: TimeInterval) {
        meteringTimerInterval = interval
    }

    public func startRecording(recordingConfig: RecordingConfig) async throws {
        let audioFilename = Constants.FilePath.audioDirectory
            .appendingPathComponent("\(Date.ios8601)")
            .appendingPathExtension(recordingConfig.audioFormat.fileExtension.rawValue)

        let settings = [
            AVFormatIDKey: Int(recordingConfig.audioFormat.audioFormatID),
            AVSampleRateKey: recordingConfig.audioSampleRate.rawValue,
            AVNumberOfChannelsKey: recordingConfig.audioChannel.rawValue,
            AVEncoderAudioQualityKey: recordingConfig.audioEncodeQuality.toAVAudioQuality.rawValue,
        ]

        await SystemSoundStaticDriver.play(systemSoundType: .startRecording)

        isEnabledAudioSessionCache = recordingConfig.isEnabledAudioSession
        if recordingConfig.isEnabledAudioSession {
            try AVAudioSession.sharedInstance().setCategory(
                recordingConfig.audioSessionCategory.toAVAudioSessionCategory,
                mode: recordingConfig.audioSessionMode.toAVAudioSessionMode,
                options: []
            )
            OSLogger.debugLog("Start: session active")
            try AVAudioSession.sharedInstance().setActive(true)
            OSLogger.debugLog("Completed: session active")
        }

        OSLogger.debugLog("Start: start recording")
        audioRecorder = try AVAudioRecorder(url: audioFilename, settings: settings)
        audioRecorder?.isMeteringEnabled = true
        audioRecorder?.prepareToRecord()

        // ä¸Šè¨˜ã¾ã§ã®å‡¦ç†ã¯ã‚¤ãƒ‹ã‚·ãƒ£ãƒ©ã‚¤ã‚¶ã§è¡Œã£ã¦ã‚‚è‰¯ã„ã‹ã‚‚ã—ã‚Œãªã„
        audioRecorder?.record()
        isRecordingSubject.send(true)
        OSLogger.debugLog("Start: start recording")

        startRecordingTimer()
        startMeteringTimer()
    }

    public func stopRecording() async throws {
        guard let audioRecorder, audioRecorder.isRecording else {
            return
        }

        stopRecordingTimer()
        stopMeteringTimer()

        OSLogger.debugLog("Start: stop recording")
        audioRecorder.stop()
        isRecordingSubject.send(false)
        OSLogger.debugLog("Completed: stop recording")

        if isEnabledAudioSessionCache {
            OSLogger.debugLog("Start: session unactive")
            try AVAudioSession.sharedInstance().setActive(false)
            OSLogger.debugLog("Completed: session unactive")
        }

        await SystemSoundStaticDriver.play(systemSoundType: .stopRecording)
    }

    private func startRecordingTimer() {
        recordingTimeSubject.value = 0

        Timer.publish(every: 1, on: .main, in: .common).autoconnect()
            .sink { [weak self] _ in
                self?.recordingTimeSubject.value += 1
            }
            .store(in: &recordingTimer)
    }

    private func stopRecordingTimer() {
        recordingTimer.forEach { $0.cancel() }
    }

    private func startMeteringTimer() {
        Timer.publish(every: meteringTimerInterval, on: .main, in: .common).autoconnect()
            .compactMap { [weak self] _ in
                guard let self else {
                    return nil
                }
                audioRecorder?.updateMeters()
                return AudioHelpStaticDriver.normalized(decibel: Double(audioRecorder?.averagePower(forChannel: 0) ?? -80))
            }
            .assign(to: \.value, on: audioVolumeLevelSubject)
            .store(in: &meteringTimer)
    }

    private func stopMeteringTimer() {
        meteringTimer.forEach { $0.cancel() }
    }
}

---

## ğŸ¤– Assistant
*2024-07-29 16:39:14*

User has a `RecordingDriver` class in Swift that uses `AVAudioRecorder` for recording audio and manages various recording states and metrics using Combine.

---

## ğŸ”§ Tool
*2024-07-29 16:39:14*

Model set context updated.

---

## ğŸ¤– Assistant
*2024-07-29 16:38:15*

åˆ¥ã®ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã€ã™ã§ã«å­˜åœ¨ã™ã‚‹`RecordingDriver`ã‚¯ãƒ©ã‚¹ã«åŸºã¥ã„ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®æ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹å®Ÿè£…ã‚’ç¤ºã—ã¾ã™ã€‚`Speech`ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦æ–‡å­—èµ·ã“ã—ã‚’è¡Œã„ã¾ã™ã€‚

ã¾ãšã€æ–°ã—ã„ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚¯ãƒ©ã‚¹ã¨ã—ã¦`TranscriptionDriver`ã‚’ä½œæˆã—ã€ãã®ä¸­ã§æ–‡å­—èµ·ã“ã—ã®æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã™ã€‚æ¬¡ã«ã€æ—¢å­˜ã®`RecordingDriver`ã‚¯ãƒ©ã‚¹ã«ã“ã®ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ±åˆã—ã¾ã™ã€‚

### 1. `TranscriptionDriver`ã®ä½œæˆ

```swift
import Speech
import Combine

public final class TranscriptionDriver: ObservableObject {
    private var speechRecognizer = SFSpeechRecognizer()
    private var recognitionRequest: SFS

---

## ğŸ‘¤ User
*2024-07-29 16:39:15*

å„Driver ã¯ç‹¬ç«‹ã—ã¦ã„ã¦ã€ãã‚Œã‚’ä½¿ã†UseCase ã® classã‚’å®šç¾©ã—ã¦ãã‚Œã§å®Ÿè£…ã—ã¦æ¬²ã—ã„ã€‚

---

## ğŸ¤– Assistant
*2024-07-29 16:39:25*

User prefers each driver to be independent and wants to define a UseCase class to implement the functionality using these drivers.

---

## ğŸ”§ Tool
*2024-07-29 16:39:25*

Model set context updated.

---

## ğŸ¤– Assistant
*2024-07-29 16:39:25*

äº†è§£ã—ã¾ã—ãŸã€‚å„Driverã‚’ç‹¬ç«‹ã•ã›ãŸã¾ã¾ã€UseCaseã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã—ã¦ã€éŒ²éŸ³ã¨æ–‡å­—èµ·ã“ã—ã®æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

### 1. TranscriptionDriver ã®ä½œæˆ

ã¾ãšã€éŸ³å£°èªè­˜ã‚’æ‹…å½“ã™ã‚‹ `TranscriptionDriver` ã‚’ä½œæˆã—ã¾ã™ã€‚

```swift
import Speech
import Combine

public final class TranscriptionDriver: ObservableObject {
    private var speechRecognizer = SFSpeechRecognizer()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    
    private let transcriptSubject = PassthroughSubject<String, Never>()
    public var transcriptPublisher: AnyPublisher<String, Never> {
        transcriptSubject.eraseToAnyPublisher()
    }
    
    public init() {}
    
    public func startTranscribing() {
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        
        guard let recognitionRequest = recognitionRequest else {
            fatalError("Unable to create an SFSpeechAudioBufferRecognitionRequest object")
        }
        
        recognitionRequest.shouldReportPartialResults = true
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
            if let result = result {
                DispatchQueue.main.async {
                    self.transcriptSubject.send(result.bestTranscription.formattedString)
                }
            }
            
            if let error = error {
                print("Error recognizing speech: \(error.localizedDescription)")
                self.stopTranscribing()
            }
        }
    }
    
    public func stopTranscribing() {
        recognitionTask?.cancel()
        recognitionTask = nil
    }
    
    public func appendAudioData(_ buffer: AVAudioPCMBuffer) {
        recognitionRequest?.append(buffer)
    }
    
    public func requestAuthorization(completion: @escaping (Bool) -> Void) {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    completion(true)
                default:
                    completion(false)
                }
            }
        }
    }
}
```

### 2. UseCase ã‚¯ãƒ©ã‚¹ã®ä½œæˆ

`RecordingDriver` ã¨ `TranscriptionDriver` ã‚’åˆ©ç”¨ã—ã¦éŒ²éŸ³ã¨æ–‡å­—èµ·ã“ã—ã®æ©Ÿèƒ½ã‚’çµ±åˆã™ã‚‹ `RecordingUseCase` ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚

```swift
import AVFoundation
import Combine

public final class RecordingUseCase: ObservableObject {
    private let recordingDriver: RecordingDriver
    private let transcriptionDriver: TranscriptionDriver
    
    private var audioEngine: AVAudioEngine
    private var audioInputNode: AVAudioInputNode?
    
    private var cancellables = Set<AnyCancellable>()
    
    @Published public var transcript: String = ""
    
    public init(recordingDriver: RecordingDriver, transcriptionDriver: TranscriptionDriver) {
        self.recordingDriver = recordingDriver
        self.transcriptionDriver = transcriptionDriver
        
        self.audioEngine = AVAudioEngine()
        
        setupBindings()
    }
    
    private func setupBindings() {
        transcriptionDriver.transcriptPublisher
            .sink { [weak self] transcript in
                self?.transcript = transcript
            }
            .store(in: &cancellables)
    }
    
    public func start() {
        transcriptionDriver.requestAuthorization { [weak self] authorized in
            guard authorized else {
                print("Speech recognition authorization denied")
                return
            }
            
            self?.transcriptionDriver.startTranscribing()
            self?.startRecording()
        }
    }
    
    private func startRecording() {
        let audioFormat = AVAudioFormat(standardFormatWithSampleRate: 44100.0, channels: 1)!
        audioInputNode = audioEngine.inputNode
        
        audioInputNode?.installTap(onBus: 0, bufferSize: 1024, format: audioFormat) { [weak self] buffer, when in
            self?.transcriptionDriver.appendAudioData(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
        } catch {
            print("AudioEngine couldn't start because of an error: \(error.localizedDescription)")
        }
        
        Task {
            try await recordingDriver.startRecording(recordingConfig: RecordingConfig())
        }
    }
    
    public func stop() {
        recordingDriver.stopRecording()
        transcriptionDriver.stopTranscribing()
        
        audioEngine.stop()
        audioInputNode?.removeTap(onBus: 0)
    }
}
```

### 3. SwiftUIãƒ“ãƒ¥ãƒ¼ã®ä½œæˆ

éŒ²éŸ³ã¨æ–‡å­—èµ·ã“ã—ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã® SwiftUI ãƒ“ãƒ¥ãƒ¼ã‚’ä½œæˆã—ã¾ã™ã€‚

```swift
import SwiftUI

struct ContentView: View {
    @StateObject var recordingUseCase = RecordingUseCase(
        recordingDriver: RecordingDriver(),
        transcriptionDriver: TranscriptionDriver()
    )
    
    var body: some View {
        VStack {
            Text(recordingUseCase.transcript)
                .padding()
                .border(Color.gray, width: 1)
            
            Button(action: {
                recordingUseCase.start()
            }) {
                Text("éŒ²éŸ³é–‹å§‹ã¨æ–‡å­—èµ·ã“ã—")
            }
            .padding()
            
            Button(action: {
                recordingUseCase.stop()
            }) {
                Text("éŒ²éŸ³åœæ­¢")
            }
            .padding()
        }
    }
}
```

ã“ã‚Œã§ã€éŒ²éŸ³ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®æ–‡å­—èµ·ã“ã—ã‚’è¡Œã†æ©Ÿèƒ½ãŒã€ç‹¬ç«‹ã—ãŸDriverã‚¯ãƒ©ã‚¹ã¨UseCaseã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦å®Ÿè£…ã•ã‚Œã¾ã—ãŸã€‚

---
